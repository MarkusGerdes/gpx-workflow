#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
3_analyze_peaks_plot.py (V6 - Handles missing Time/Elevation Data)
-----------------------
Analyzes track data from a CSV file (generated by 2_parse_gpx_full.py)
to find significant peaks and their associated segments. Calculates
overall and time-based statistics (if time is available). Generates a
detailed elevation profile plot with slope coloring and optional place
annotations. Saves results to separate files.
"""

# ... (Imports wie vorher, stelle sicher, dass Optional und KDTree da sind) ...
import sys
import os
import argparse
from dataclasses import dataclass, field
from typing import List, Tuple, Optional, Set, Dict
import time

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from matplotlib.collections import LineCollection
from matplotlib.colors import ListedColormap, BoundaryNorm
from matplotlib.patches import Patch
import numpy as np
import pandas as pd
from scipy.signal import savgol_filter, find_peaks
from geopy.distance import distance as geopy_distance
from scipy.spatial import KDTree

# ------------------------------------------------------------
#  Konfigurationsobjekt (ERWEITERT f√ºr Orts-Offset)
# ------------------------------------------------------------
@dataclass
class Config:
    # ... (Alle bisherigen Config-Werte) ...
    smooth_window: int = 11
    smooth_poly: int = 2
    gain_threshold: float = 30.0
    eps_height: float = 0.3
    min_peak_prominence_m: float = 40.0
    peak_edge_km: float = 0.25
    plot_dpi: int = 150
    plot_x_tick_major: float = 5.0
    plot_x_tick_minor: float = 1.0
    min_length_draw_m: float = 100.0
    color_fwd_valid_label: str = "#1f77b4"
    color_bwd_valid_label: str = "#2ca02c"
    color_peak1: str = "#ff7f0e"
    color_peak2: str = "#9467bd"
    color_fwd_shade: str = "#d6ebf2"
    color_bwd_shade: str = "#d8f0d3"
    shade_alpha: float = 0.4
    color_invalid: str = "#cccccc"
    invalid_alpha: float = 0.20
    slope_thresholds: List[float] = field(default_factory=lambda: [-100, -12, -8, -5, -2, -0.5, 0.5, 2, 5, 8, 12, 100])
    slope_colors: List[str] = field(default_factory=lambda: ['#08306b', '#08519c', '#3182bd', '#9ecae1', '#deebf7','#d9d9d9','#e5f5e0', '#a1d99b', '#fed976', '#fd8d3c', '#e31a1c'])
    slope_labels: List[str] = field(default_factory=lambda: ["<-12%", "-12..-8%", "-8..-5%", "-5..-2%", "-2..-0.5%","Flat","0.5..2%", "2..5%", "5..8%", "8..12%", ">12%"])
    slope_linewidth: float = 2.0
    pause_min_duration_s: float = 120.0
    pause_max_distance_m: float = 5.0
    place_marker_style: str = '^'
    place_marker_color: str = 'darkviolet'
    place_marker_size: int = 8
    place_text_color: str = 'darkviolet'
    place_text_size: int = 7
    place_text_offset_y: int = 12 # Basis-Offset
    place_text_bg_alpha: float = 0.7

    # --- NEU: Konfiguration f√ºr dynamischen Y-Offset der Ortslabels ---
    # Distanz-Bins (obere Grenze in Metern)
    place_offset_dist_bins_m: List[float] = field(default_factory=lambda: [100, 300, 600, 1000, 2000])
    # Zus√§tzlicher Y-Offset (in Plot-Einheiten, also Metern H√∂he) f√ºr jeden Bin
    # Muss eine Liste mit len(bins)+1 Elementen sein (Wert f√ºr <bin[0], bin[0]-bin[1], ..., >bin[-1])
    place_offset_y_additions: List[float] = field(default_factory=lambda: [0, 8, 18, 30, 45, 60])
    # --------------------------------------------------------------------

# ------------------------------------------------------------
#  Datenklasse Segment (unver√§ndert)
# ------------------------------------------------------------
@dataclass
class Segment:
    peak_rank: int
    start_idx: int
    end_idx: int
    gain_m: float
    length_m: float
    direction: str # 'forward' or 'backward'

    @property
    def valid(self) -> bool:
        # IMPORTANT: Access threshold via the *instance* of Config passed around
        # Assuming the Config object is available in the scope where this is checked
        # This might need adjustment depending on how Config is passed.
        # For now, assuming a global or passed config object.
        # A better way would be to pass config to the Segment or check externally.
        # Let's assume it's checked externally for now, or Config needs to be global/passed.
        # Example if passed: def valid(self, config: Config) -> bool: ...
        # Hardcoding for now, needs refinement if Config isn't easily accessible here.
        return self.gain_m >= 30.0 # Fallback to hardcoded value

    def get_index_range(self) -> Set[int]:
        return set(range(self.start_idx, self.end_idx + 1))

# ------------------------------------------------------------
#  Helfer (angepasst f√ºr Config-Instanz)
# ------------------------------------------------------------
# smooth_elev (unver√§ndert)
def smooth_elev(elev: np.ndarray, config: Config) -> np.ndarray:
    if len(elev) < config.smooth_window: return elev.copy()
    # Ensure window is odd and smaller than array length
    window = config.smooth_window if config.smooth_window % 2 != 0 else config.smooth_window + 1
    if window > len(elev):
        window = len(elev) if len(elev) % 2 != 0 else len(elev) - 1
        if window < 3: return elev.copy() # Need at least 3 points for poly order 2
    poly = config.smooth_poly
    # Ensure poly order is less than window size
    if poly >= window:
        poly = window - 1
        if poly < 1: poly = 1 # Minimum poly order is 1
    try:
        return savgol_filter(elev, window, poly)
    except ValueError as e:
        print(f"[Warnung] Fehler bei Savgol-Filter (window={window}, poly={poly}, len={len(elev)}): {e}. Ungegl√§ttete Daten werden verwendet.")
        return elev.copy()

# ------------------------------------------------------------
#  Analyse‚ÄëRoutinen (angepasst f√ºr Config-Instanz)
# ------------------------------------------------------------
# analyse_direction (unver√§ndert)
def analyse_direction(
    elev: np.ndarray, dist: np.ndarray, peak_idx: int, direction: str,
    peak_rank: int, n: int, config: Config, excluded_indices: Optional[Set[int]] = None
) -> Tuple[List[Segment], List[Segment]]:
    """Analyzes segments in one direction from a peak."""
    if not (0 <= peak_idx < n): return [], []
    excluded_indices = excluded_indices or set()
    step = 1 if direction == "forward" else -1
    i = peak_idx + step

    mode = "falling" # Start assuming we are falling from the peak
    current_gain = 0.0
    segment_start_idx = -1 # Index where the current potential rising segment started

    all_found: List[Segment] = []
    first_valid: List[Segment] = []

    while 0 <= i < n:
        if i in excluded_indices:
            # If we hit an excluded index while rising, reset the segment
            if mode == "rising":
                mode = "falling"
                current_gain = 0.0
                segment_start_idx = -1
            i += step
            continue

        # Get previous index, ensure it's valid and not excluded
        prev_idx = i - step
        if not (0 <= prev_idx < n): break # Should not happen if loop condition is correct
        if prev_idx in excluded_indices:
            # If the previous point was excluded, we can't calculate diff, reset if rising
            if mode == "rising":
                 mode = "falling"
                 current_gain = 0.0
                 segment_start_idx = -1
            i += step
            continue

        # Calculate elevation difference
        diff = elev[i] - elev[prev_idx]

        # Ignore negligible height changes
        if abs(diff) < config.eps_height:
            i += step
            continue

        if mode == "falling":
            # If we start rising, mark the beginning of a potential segment
            if diff > 0:
                mode = "rising"
                segment_start_idx = prev_idx
                current_gain = diff
        else: # mode == "rising"
            # Continue accumulating gain if still rising
            if diff > 0:
                current_gain += diff
            # If we start falling, the rising segment ends
            else:
                segment_end_idx = prev_idx
                # Only record if a valid start was found
                if segment_start_idx != -1:
                    length_m = abs(dist[segment_end_idx] - dist[segment_start_idx])
                    # Ensure indices are ordered correctly for the Segment object
                    idx1, idx2 = min(segment_start_idx, segment_end_idx), max(segment_start_idx, segment_end_idx)
                    # Pass config.gain_threshold to check validity explicitly if needed
                    # For now, rely on the property check later
                    seg = Segment(peak_rank, idx1, idx2, current_gain, length_m, direction)
                    all_found.append(seg)
                    # Check validity using the threshold from config
                    if seg.gain_m >= config.gain_threshold and not first_valid:
                        first_valid.append(seg)
                        # break # Optional: Stop after finding the first valid segment
                # Reset for the next potential segment
                mode = "falling"
                current_gain = 0.0
                segment_start_idx = -1
        i += step

    # Handle case where a rising segment goes to the very end/start of the track
    if mode == "rising" and segment_start_idx != -1:
         segment_end_idx = i - step # The last valid index processed
         length_m = abs(dist[segment_end_idx] - dist[segment_start_idx])
         idx1, idx2 = min(segment_start_idx, segment_end_idx), max(segment_start_idx, segment_end_idx)
         seg = Segment(peak_rank, idx1, idx2, current_gain, length_m, direction)
         all_found.append(seg)
         if seg.gain_m >= config.gain_threshold and not first_valid:
             first_valid.append(seg)


    return all_found, first_valid

# analyze_single_peak (unver√§ndert)
def analyze_single_peak(
    peak_idx: int, peak_rank: int, elev: np.ndarray, dist: np.ndarray,
    track_len_km: float, n: int, config: Config, excluded_indices: Optional[Set[int]] = None
) -> Tuple[List[Segment], Set[int]]:
    """Analyzes segments forward and backward from a single peak."""
    peak_dist_km = dist[peak_idx] / 1000.0
    peak_elev_m = elev[peak_idx]
    segments: List[Segment] = []
    valid_segment_indices: Set[int] = set()

    print(f"  Analysiere Peak {peak_rank} ({peak_elev_m:.1f} m @ {peak_dist_km:.2f} km):")

    # Check if peak is too close to the edges
    is_peak_near_start = peak_dist_km <= config.peak_edge_km
    is_peak_near_end = (track_len_km - peak_dist_km) <= config.peak_edge_km

    # --- Forward Analysis ---
    if is_peak_near_end:
        print(f"    -> P{peak_rank} am Ende - keine Vorw√§rtsanalyse.")
    else:
        all_fwd, valid_fwd = analyse_direction(elev, dist, peak_idx, "forward", peak_rank, n, config, excluded_indices)
        segments.extend(all_fwd)
        # Check validity explicitly using config
        valid_fwd_filtered = [s for s in valid_fwd if s.gain_m >= config.gain_threshold]
        if valid_fwd_filtered:
            s = valid_fwd_filtered[0]
            end_hint = " (-> Ende)" if s.end_idx == n - 1 else ""
            print(f"    -> OK P{peak_rank} Vorw√§rts: +{s.gain_m:.1f} m / {s.length_m:.0f} m{end_hint}")
            valid_segment_indices.update(s.get_index_range())
        else:
            print(f"    -> !! P{peak_rank} Vorw√§rts: Kein signif. Segment.")

    # --- Backward Analysis ---
    if is_peak_near_start:
        print(f"    <- P{peak_rank} am Start - keine R√ºckw√§rtsanalyse.")
    else:
        all_bwd, valid_bwd = analyse_direction(elev, dist, peak_idx, "backward", peak_rank, n, config, excluded_indices)
        segments.extend(all_bwd)
        # Check validity explicitly using config
        valid_bwd_filtered = [s for s in valid_bwd if s.gain_m >= config.gain_threshold]
        if valid_bwd_filtered:
            s = valid_bwd_filtered[0]
            start_hint = " (<- Start)" if s.start_idx == 0 else ""
            print(f"    <- OK P{peak_rank} R√ºckw√§rts: +{s.gain_m:.1f} m / {s.length_m:.0f} m{start_hint}")
            valid_segment_indices.update(s.get_index_range())
        else:
            print(f"    <- !! P{peak_rank} R√ºckw√§rts: Kein signif. Segment.")

    return segments, valid_segment_indices


# ------------------------------------------------------------
#  Statistik-Berechnung (angepasst f√ºr Config Instanz bei Pausen)
# ------------------------------------------------------------
def calculate_statistics(df: pd.DataFrame, config: Config) -> Dict[str, any]: # Verwende Any f√ºr flexible Typen
    """Calculates overall and time-based statistics. Returns None for unavailable stats."""
    stats = {}
    n_points = len(df)
    if n_points < 2: return {"Fehler": "Zu wenig Datenpunkte"}

    # --- Basic Stats ---
    stats["Gesamtdistanz (km)"] = df["Distanz (km)"].iloc[-1] if "Distanz (km)" in df.columns else None # Verwende None statt 0.0

    stats["Gesamtdistanz (km)"] = df["Distanz (km)"].iloc[-1] if "Distanz (km)" in df.columns else None
    has_elevation = 'Elevation (m)' in df.columns and df['Elevation (m)'].notna().any()
    if has_elevation:
        stats["Minimalh√∂he (m)"] = df["Elevation (m)"].min(); stats["Maximalh√∂he (m)"] = df["Elevation (m)"].max()
        stats["Gesamter Aufstieg (m)"] = df["Aufstieg (m)"].sum() if 'Aufstieg (m)' in df.columns else None
        elevation_diff = df['Elevation (m)'].diff().fillna(0); stats["Gesamter Abstieg (m)"] = abs(elevation_diff.clip(upper=0).sum())
    else:
        stats["Minimalh√∂he (m)"] = None; stats["Maximalh√∂he (m)"] = None; stats["Gesamter Aufstieg (m)"] = None; stats["Gesamter Abstieg (m)"] = None

    # --- Time Stats (Nur wenn Zeitdaten vorhanden) ---
    if 'Time' in df.columns and df['Time'].notna().any(): # Ebene 1 if
        df_time_col = pd.to_datetime(df['Time'], errors='coerce')

        if df_time_col.notna().any(): # Ebene 2 if
            valid_times_df = df_time_col.dropna()
            if len(valid_times_df) >= 2: # Ebene 3 if
                start_time = valid_times_df.iloc[0]
                end_time = valid_times_df.iloc[-1]
                total_duration_td = end_time - start_time
                stats["Gesamtdauer"] = str(total_duration_td).split('.')[0]

                pause_duration_s = 0.0
                df_temp_pause = df[['Time', 'Strecke Delta (km)']].copy()
                df_temp_pause['Time'] = pd.to_datetime(df_temp_pause['Time'], errors='coerce')
                df_temp_pause.dropna(subset=['Time'], inplace=True)

                if len(df_temp_pause) >=2: # Ebene 4 if
                    df_temp_pause['TimeDelta (s)'] = df_temp_pause['Time'].diff().dt.total_seconds().fillna(0)
                    df_temp_pause['DistDelta (m)'] = df_temp_pause['Strecke Delta (km)'] * 1000
                    for i in range(1, len(df_temp_pause)):
                        time_diff = df_temp_pause['TimeDelta (s)'].iloc[i]
                        dist_diff = df_temp_pause['DistDelta (m)'].iloc[i]
                        if time_diff >= config.pause_min_duration_s and dist_diff <= config.pause_max_distance_m:
                            pause_duration_s += time_diff

                    moving_duration_s = max(0, total_duration_td.total_seconds() - pause_duration_s)
                    stats["Pausenzeit"] = str(pd.to_timedelta(pause_duration_s, unit='s')).split('.')[0]
                    stats["Bewegungszeit"] = str(pd.to_timedelta(moving_duration_s, unit='s')).split('.')[0]

                    total_duration_h = total_duration_td.total_seconds() / 3600
                    moving_duration_h = pd.to_timedelta(moving_duration_s, unit='s').total_seconds() / 3600
                    dist_km_val = stats.get("Gesamtdistanz (km)")
                    dist_km = float(dist_km_val) if dist_km_val is not None else 0.0

                    stats["√ò Geschwindigkeit (km/h)"] = (dist_km / total_duration_h) if total_duration_h > 0 else None
                    stats["√ò Geschw. in Bewegung (km/h)"] = (dist_km / moving_duration_h) if moving_duration_h > 0 else None
                else: # Geh√∂rt zu Ebene 4 if
                    print("[Warnung] Nicht genug g√ºltige Zeitpunkte f√ºr Pausenberechnung.")
                    stats["Pausenzeit"] = None; stats["Bewegungszeit"] = None
                    stats["√ò Geschwindigkeit (km/h)"] = None; stats["√ò Geschw. in Bewegung (km/h)"] = None
            else: # Geh√∂rt zu Ebene 3 if
                print("[Warnung] Nicht genug g√ºltige Zeitpunkte f√ºr Gesamtdauer.")
                stats["Gesamtdauer"] = None; stats["Pausenzeit"] = None; stats["Bewegungszeit"] = None
                stats["√ò Geschwindigkeit (km/h)"] = None; stats["√ò Geschw. in Bewegung (km/h)"] = None
        else: # Geh√∂rt zu Ebene 2 if
            print("[Info] Keine validen Zeitdaten f√ºr Statistikberechnung nach Konvertierung gefunden.")
            stats["Gesamtdauer"] = None; stats["Pausenzeit"] = None; stats["Bewegungszeit"] = None
            stats["√ò Geschwindigkeit (km/h)"] = None; stats["√ò Geschw. in Bewegung (km/h)"] = None
    else: # Geh√∂rt zu Ebene 1 if
        print("[Info] Keine Zeitdaten-Spalte f√ºr Statistikberechnung gefunden.")
        stats["Gesamtdauer"] = None; stats["Pausenzeit"] = None; stats["Bewegungszeit"] = None
        stats["√ò Geschwindigkeit (km/h)"] = None; stats["√ò Geschw. in Bewegung (km/h)"] = None    # ---------------------------------------------------

    # Format Floats und entferne Nones
    stats_formatted = {}
    for key, value in stats.items():
        if value is not None:
            if isinstance(value, (float, np.float64)):
                stats_formatted[key] = f"{value:.2f}"
            else:
                stats_formatted[key] = value # Strings (wie formatierte Dauer) bleiben erhalten

    return stats_formatted
# ------------------------------------------------------------
#  Plot‚ÄëFunktionen (ERWEITERT f√ºr Orte)
# ------------------------------------------------------------
# _calculate_slope_colors (unver√§ndert)
def _calculate_slope_colors(dist_m: np.ndarray, elev_m: np.ndarray, config: Config) -> Tuple[np.ndarray, ListedColormap, BoundaryNorm]:
    """Berechnet Steigungsprozente und weist Farben basierend auf Schwellen zu."""
    if len(dist_m) < 2: return np.array([]), ListedColormap([]), BoundaryNorm([], 0)

    d_elev = np.gradient(elev_m)
    d_dist = np.gradient(dist_m)

    slope_percent = np.zeros_like(d_dist)
    min_dist_step = 1e-1
    valid_dist_mask = d_dist > min_dist_step

    slope_percent[valid_dist_mask] = (d_elev[valid_dist_mask] / d_dist[valid_dist_mask]) * 100
    slope_percent = np.clip(slope_percent, config.slope_thresholds[0], config.slope_thresholds[-1])

    cmap = ListedColormap(config.slope_colors)
    norm = BoundaryNorm(config.slope_thresholds, cmap.N)
    slope_indices = np.digitize(slope_percent[:-1], config.slope_thresholds[1:], right=False)

    return slope_indices, cmap, norm

# _shade_segment (unver√§ndert)
def _shade_segment(ax, dist_km: np.ndarray, elev_m: np.ndarray, seg: Segment, config: Config):
    """Zeichnet Hintergrundschattierungen und Labels f√ºr Segmente."""
    # Use config.gain_threshold to check validity
    is_valid = seg.gain_m >= config.gain_threshold

    if is_valid:
        shade_color = config.color_fwd_shade if seg.direction == "forward" else config.color_bwd_shade
        shade_alpha = config.shade_alpha
        text_color = config.color_fwd_valid_label if seg.direction == "forward" else config.color_bwd_valid_label
    else: # Invalid segment
        shade_color = config.color_invalid
        shade_alpha = config.invalid_alpha
        text_color = "#666666" # Muted color for invalid labels if shown

    x0_km, x1_km = dist_km[seg.start_idx], dist_km[seg.end_idx]
    segment_width_km = abs(x1_km - x0_km)

    # Draw shading if valid or long enough invalid
    if is_valid or seg.length_m >= config.min_length_draw_m:
         ax.axvspan(x0_km, x1_km, color=shade_color, alpha=shade_alpha, zorder=1, lw=0)

         # Draw text label ONLY for VALID segments and if segment is wide enough on plot
         if is_valid and segment_width_km > 0.5: # Only label if wider than 0.5km on plot
            text_x_km = (x0_km + x1_km) / 2
            seg_indices = range(seg.start_idx, seg.end_idx + 1)
            # --- KORREKTUR HIER ---
            if not list(seg_indices): return # Use return to exit function if range is empty
            # ----------------------
            text_y_max_in_segment = np.max(elev_m[list(seg_indices)]) # Ensure indices are list for numpy < 1.25
            y_offset = 5
            ax.text(text_x_km, text_y_max_in_segment + y_offset, f"P{seg.peak_rank}:{seg.gain_m:.0f}m",
                    ha="center", va="bottom", fontsize=8,
                    color=text_color, weight='bold',
                    bbox=dict(boxstyle="round,pad=0.15", fc="white", ec="none", alpha=0.75),
                    zorder=5)


# --- N√§chsten Nachbarn finden (ERWEITERT: gibt Distanz zur√ºck) ---
def find_nearest_track_point_kdtree(track_lat_lon: np.ndarray, place_lat: float, place_lon: float) -> Tuple[int, float]:
    """
    Finds the index and approximate distance (in meters) of the nearest track point using KDTree.
    Assumes track_lat_lon is an Nx2 array with [Latitude, Longitude].
    Returns (index, distance_meters) or (-1, -1.0) on error.
    """
    if track_lat_lon is None or track_lat_lon.shape[0] == 0:
        return -1, -1.0
    try:
        tree = KDTree(track_lat_lon)
        distance_deg, index = tree.query([place_lat, place_lon])

        # Konvertiere Distanz von Grad zu Metern (Ann√§herung!)
        # Genauere Methode: Geopy verwenden, nachdem der Index gefunden wurde.
        nearest_track_point = track_lat_lon[index]
        try:
            # Verwende geopy f√ºr genauere Distanzberechnung
            distance_m = geopy_distance((place_lat, place_lon), (nearest_track_point[0], nearest_track_point[1])).meters
        except ValueError:
             # Fallback auf Grad-Umrechnung bei geopy-Fehler
             print("[Warnung] Geopy-Distanzberechnung fehlgeschlagen, nutze Grad-Approximation.")
             distance_m = distance_deg * 111000 # Grobe Ann√§herung

        return index, distance_m
    except Exception as e:
        print(f"[Warnung] KDTree Fehler: {e}")
        return -1, -1.0


# --- plot_profile (ERWEITERT um dynamischen Orts-Offset) ---
def plot_profile(base_filename: str,
                 track_df: pd.DataFrame,
                 peak_indices: List[int],
                 all_segments: List[Segment],
                 config: Config,
                 output_plot_path: str,
                 places_coords_df: Optional[pd.DataFrame] = None,
                 water_pois_to_plot_df: Optional[pd.DataFrame] = None): # NEUER Parameter
    """Erstellt Plot mit optionalen Ortsmarkern mit dynamischem Y-Offset."""
    print(f"[Info] Erstelle Plot: {output_plot_path}")
    fig, ax = plt.subplots(figsize=(14, 7.5))

    # ... (Daten extrahieren, Plot initialisieren, Slope Colors, Peaks, Segments wie vorher) ...
    # Extrahiere Daten aus DataFrame
    dist_m = track_df['Distanz (km)'].values * 1000.0
    elev_m = track_df['Elevation (m)'].values # Verwende gegl√§ttete H√∂he f√ºr Peaks/Segmente, aber rohe f√ºr Plot? Nein, gegl√§ttet ist besser f√ºr Steigung.
    dist_km = dist_m / 1000.0
    n_points = len(dist_m)

    if n_points < 2:
        print("[Warnung] Weniger als 2 Punkte f√ºr Plot vorhanden.")
        plt.close(fig) # Schlie√üe die leere Figur

        # --- KORREKTUR: Try-Except auf mehrere Zeilen aufteilen ---
        try:
            # Sicherstellen, dass der Output-Ordner existiert
            output_dir = os.path.dirname(output_plot_path)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)
            # Erstelle leere Ausgabedatei
            open(output_plot_path, 'a').close()
            print(f"[Info] Leere Plot-Datei erstellt: {output_plot_path}")
        except Exception as e:
            # Fehler beim Erstellen der Dummy-Datei ignorieren oder loggen
            print(f"[Warnung] Konnte leere Plot-Datei nicht erstellen: {e}")
        return # Verlasse die Funktion plot_profile fr√ºhzeitig

    # 1. Slope Colors & LineCollection
    slope_indices, cmap, norm = _calculate_slope_colors(dist_m, elev_m, config)
    if len(slope_indices) == 0:
        ax.plot(dist_km, elev_m, color='black', linewidth=1.5, zorder=2, label='H√∂henprofil (Fallback)')
    else:
        points = np.array([dist_km, elev_m]).T.reshape(-1, 1, 2)
        segments_lc = np.concatenate([points[:-1], points[1:]], axis=1)
        lc = LineCollection(segments_lc, cmap=cmap, norm=norm, linewidth=config.slope_linewidth, zorder=2)
        lc.set_array(slope_indices)
        line = ax.add_collection(lc)
        ax.set_xlim(dist_km.min(), dist_km.max())
        ax.set_ylim(elev_m.min() - 10, elev_m.max() + 30)

    # 3. Mark Peaks
    # ... (Code zum Markieren der Peaks wie vorher) ...
    peak_colors = [config.color_peak1, config.color_peak2]
    peak_handles = []; peak_labels_text = []
    for i, peak_idx in enumerate(peak_indices):
        if i >= len(peak_colors): break
        peak_x_km, peak_y_m = dist_km[peak_idx], elev_m[peak_idx]
        color = peak_colors[i]; label = f"Peak {i+1}"
        peak_labels_text.append(label)
        handle = plt.Line2D([0], [0], color=color, linestyle='--', linewidth=1.5, label=label)
        peak_handles.append(handle)
        ax.axvline(peak_x_km, color=color, linestyle="--", linewidth=1.5, zorder=3, alpha=0.8)
        ax.plot(peak_x_km, peak_y_m, 'o', color=color, markersize=7, zorder=4, mec='black', mew=0.5)
        ax.text(peak_x_km + 0.15 * (ax.get_xlim()[1] - ax.get_xlim()[0])/100 ,
                peak_y_m, f"{peak_y_m:.0f} m", color='black', fontsize=8, va='center', ha='left',
                bbox=dict(boxstyle="round,pad=0.15", fc=color, ec="black", lw=0.5, alpha=0.85), zorder=6)

    # 4. Shade Segments
    for seg in all_segments:
        is_valid = seg.gain_m >= config.gain_threshold
        _shade_segment(ax, dist_km, elev_m, seg, config) # √úbergibt jetzt valides Segment


    # ***** 5. Orte annotieren (mit dynamischem Offset) *****
    place_annotations = [] # Liste zum Speichern der Annotationsdetails
    plotted_place_names = set() # Um doppelte Labels zu vermeiden, falls Orte nah beieinander liegen
    if places_coords_df is not None and not places_coords_df.empty:
        print("[Info] F√ºge Ortsmarker zum Plot hinzu...")
        track_coords_latlon = track_df[['Latitude', 'Longitude']].values
        if track_coords_latlon.shape[0] > 0:
            # Bereite Bins und Offsets vor
            dist_bins = np.array(config.place_offset_dist_bins_m)
            y_offsets = np.array(config.place_offset_y_additions)
            if len(y_offsets) != len(dist_bins) + 1:
                 print("[Warnung] L√§nge von place_offset_y_additions passt nicht zu place_offset_dist_bins_m! Verwende Basis-Offset.")
                 use_dynamic_offset = False
            else:
                 use_dynamic_offset = True

            for _, place_row in places_coords_df.iterrows():
                place_name = place_row['Ort']
                place_lat = place_row.get('Latitude_Center')
                place_lon = place_row.get('Longitude_Center')

                if pd.notna(place_lat) and pd.notna(place_lon):
                    nearest_idx, distance_m = find_nearest_track_point_kdtree(track_coords_latlon, place_lat, place_lon) # Erhalte auch Distanz

                    if nearest_idx != -1:
                        track_point = track_df.iloc[nearest_idx]
                        plot_dist_km = track_point['Distanz (km)']
                        plot_elev_m = track_point['Elevation (m)']

                        # Berechne zus√§tzlichen Y-Offset basierend auf Distanz-Binning
                        additional_offset_y = 0
                        if use_dynamic_offset:
                            # Finde den Index des Bins, in den die Distanz f√§llt
                            # np.digitize gibt Index des Bins zur√ºck (beginnend bei 1)
                            # Indices sind 0 (<=bin[0]), 1 (bin[0]<..<bin[1]), ..., N (>=bin[N-1])
                            bin_index = np.digitize(distance_m, dist_bins)
                            additional_offset_y = y_offsets[bin_index]

                        total_y_offset = config.place_text_offset_y + additional_offset_y

                        # Speichere f√ºr sp√§teres Plotten
                        place_annotations.append({
                            'name': place_name,
                            'x': plot_dist_km,
                            'y': plot_elev_m,
                            'y_offset': total_y_offset,
                            'distance_m': distance_m # F√ºr Debugging oder sp√§tere Verwendung
                        })
                    # ... (Warnung, wenn Punkt nicht gefunden) ...
                # ... (Warnung bei fehlenden Koordinaten) ...

            # Zeichne Marker und Texte f√ºr Orte
            # Sortiere Annotationen nach X-Position, um √úberlappung besser handhaben zu k√∂nnen (optional)
            place_annotations.sort(key=lambda p: p['x'])
            last_label_x_end = -np.inf # Um √úberlappung von Textboxen zu pr√ºfen
            label_height = 15 # Gesch√§tzte H√∂he einer Textbox in Plot-Koordinaten

            for anno in place_annotations:
                 # Marker zeichnen
                 ax.plot(anno['x'], anno['y'],
                        marker=config.place_marker_style, color=config.place_marker_color,
                        markersize=config.place_marker_size, linestyle='None', zorder=5)

                 # Text zeichnen (mit √úberlappungspr√ºfung - einfach)
                 # Wenn das neue Label horizontal mit dem alten √ºberlappt, erh√∂he Y weiter
                 current_label_y = anno['y'] + anno['y_offset']
                 # Einfache horizontale √úberlappungspr√ºfung (k√∂nnte verfeinert werden)
                 # if anno['x'] < last_label_x_end:
                 #      current_label_y += label_height # Schiebe es weiter hoch

                 ax.text(anno['x'], current_label_y, anno['name'],
                        color=config.place_text_color, fontsize=config.place_text_size,
                        ha='center', va='bottom',
                        bbox=dict(boxstyle="round,pad=0.15", fc="white", ec=config.place_marker_color, lw=0.5, alpha=config.place_text_bg_alpha),
                        zorder=6)
                 # Aktualisiere die Position des letzten Labels (vereinfacht)
                 # TODO: Pr√§zisere BBox-Kollisionserkennung w√§re komplexer
                 # last_label_x_end = anno['x'] + (len(anno['name']) * 0.1) # Grobe Sch√§tzung der Textbreite

        # ... (Warnung KDTree nicht verf√ºgbar) ...

    # ***** NEU: 5a Wasserstellen annotieren *****
    water_poi_annotations = []
    if water_pois_to_plot_df is not None and not water_pois_to_plot_df.empty:
        print("[Info] F√ºge Wasserstellen-Marker zum Plot hinzu...")
        track_coords_latlon = track_df[['Latitude', 'Longitude']].values # Wiederverwenden oder neu holen
        
        if track_coords_latlon.shape[0] > 0:
            for _, poi_row in water_pois_to_plot_df.iterrows():
                poi_name = poi_row.get('Name', 'Wasser') # Fallback-Name
                poi_lat = poi_row.get('Latitude')
                poi_lon = poi_row.get('Longitude')

                if pd.notna(poi_lat) and pd.notna(poi_lon):
                    nearest_idx, distance_to_track_m = find_nearest_track_point_kdtree(
                        track_coords_latlon, poi_lat, poi_lon
                    )

                    if nearest_idx != -1:
                        track_point = track_df.iloc[nearest_idx]
                        plot_dist_km = track_point['Distanz (km)']
                        # Y-Position f√ºr Wasserstellen: kann auf Track-H√∂he oder fest sein
                        # plot_elev_m_poi = track_point['Elevation (m)'] # Auf Track-H√∂he
                        
                        # F√ºr eine konsistente Darstellung etwas unterhalb der Hauptlinie:
                        min_elevation_on_plot = ax.get_ylim()[0]
                        plot_elev_m_poi_fixed = min_elevation_on_plot + 10 # Knapp √ºber dem unteren Rand (anpassen)
                        
                        water_poi_annotations.append({
                            'name': poi_name,
                            'x': plot_dist_km,
                            'y_marker': plot_elev_m_poi_fixed, # Y f√ºr den Marker
                            # 'y_text': plot_elev_m_poi_fixed - 5, # Y f√ºr Text (optional, falls ben√∂tigt)
                            'distance_to_track_m': distance_to_track_m
                        })
                    else:
                        print(f"[Warnung Plot] N√§chster Trackpunkt f√ºr Wasserstelle '{poi_name}' nicht gefunden.")
                else:
                    print(f"[Warnung Plot] Fehlende Koordinaten f√ºr Wasserstelle '{poi_name}'.")
        else:
            print("[Warnung Plot] Keine Track-Koordinaten f√ºr KDTree vorhanden (Wasserstellen).")

    # NEU: Wasserstellen plotten
    if water_poi_annotations:
        water_marker_color = 'deepskyblue'
        water_marker_style = 'o' # runder Marker
        water_marker_size = 3
        
        for anno in water_poi_annotations:
            ax.plot(anno['x'], anno['y_marker'],
                    marker=water_marker_style, color=water_marker_color,
                    markersize=water_marker_size, linestyle='None', zorder=5,
                    markeredgecolor=None, mew=0.5)
            # Optional: Text f√ºr Wasserstellen (kann schnell un√ºbersichtlich werden)
            # ax.text(anno['x'], anno['y_marker'] - 8, "üíß", # oder anno['name']
            #         color=water_marker_color, fontsize=7,
            #         ha='center', va='top', zorder=6)

    # ... (Achsen, Ticks, Titel, Grid, Legende wie vorher, aber Legende anpassen) ...
    # 6. Achsen, Ticks, Titel, Grid setup
    ax.set_xlabel("Distanz (km)"); ax.set_ylabel("H√∂he (m)")
    ax.set_title(f"Analyse (Steigung, 2 Peaks{' & Orte' if place_annotations else ''}) ‚Äì {base_filename}", pad=15)
    ax.grid(True, which='major', linestyle='-', linewidth='0.4', color='lightgray', zorder=0)
    ax.grid(True, which='minor', linestyle=':', linewidth='0.3', color='#ebebeb', zorder=0)
    ax.xaxis.set_major_locator(ticker.MultipleLocator(config.plot_x_tick_major))
    ax.xaxis.set_minor_locator(ticker.MultipleLocator(config.plot_x_tick_minor))
    ax.yaxis.set_minor_locator(ticker.AutoMinorLocator()); ax.tick_params(axis='both', which='major', labelsize=9)

    # 7. Legende
    slope_handles = [Patch(color=color, label=label) for color, label in zip(config.slope_colors, config.slope_labels)]
    
    place_handle = []
    if place_annotations: # Nur wenn Orte auch wirklich geplottet wurden (Liste nicht leer)
         place_handle = [plt.Line2D([0], [0], marker=config.place_marker_style, color=config.place_marker_color, label='Ort', linestyle='None', markersize=config.place_marker_size)]
    
    water_handle = []
    if water_poi_annotations: # Nur wenn Wasserstellen geplottet wurden
        water_handle = [plt.Line2D([0], [0], marker=water_marker_style, color=water_marker_color, label='Wasserstelle', linestyle='None', markersize=water_marker_size)]

    all_handles = peak_handles + place_handle + water_handle + slope_handles
    
    all_labels = peak_labels_text
    if place_annotations: all_labels.append('Ort')
    if water_poi_annotations: all_labels.append('Wasserstelle')
    all_labels.extend(config.slope_labels)
    
    num_legend_items = len(all_handles)
    ncol = 6 if num_legend_items > 15 else 5 if num_legend_items > 10 else 4 # Angepasste Spaltenzahl
    fig.legend(handles=all_handles, labels=all_labels, loc='lower center', bbox_to_anchor=(0.5, 0.01),
               ncol=ncol, fontsize=8, title="Legende", title_fontsize=9)
    fig.tight_layout(rect=[0, 0.08, 1, 0.96])

    # 8. Save plot
    # ... (Speichern wie vorher) ...
    try:
        output_dir = os.path.dirname(output_plot_path)
        if output_dir: os.makedirs(output_dir, exist_ok=True)
        fig.savefig(output_plot_path, dpi=config.plot_dpi, bbox_inches='tight')
        print(f"  -> Plot gespeichert: '{output_plot_path}'")
    except Exception as e:
        print(f"  -> Fehler beim Speichern des Plots '{output_plot_path}': {e}")
    finally:
        plt.close(fig)


# ... (main Funktion muss angepasst werden, um die Config zu nutzen) ...
def main(args):
    """Hauptfunktion: Verarbeitet eine CSV-Datei, erstellt Plot und Daten-CSVs."""
    print("Starte Peak/Profil Analyse V5 (mit dynamischem Orts-Offset)...")

    # --- Lade Konfiguration aus Argumenten und Defaults ---
    config = Config(
        smooth_window=args.smooth_window, smooth_poly=args.smooth_poly,
        gain_threshold=args.gain_threshold, eps_height=args.eps_height,
        min_peak_prominence_m=args.prominence, peak_edge_km=args.peak_edge_km,
        plot_dpi=args.plot_dpi, plot_x_tick_major=args.plot_x_tick_major,
        plot_x_tick_minor=args.plot_x_tick_minor,
        pause_min_duration_s=args.pause_min_duration,
        pause_max_distance_m=args.pause_max_distance
        # Die neuen Offset-Parameter werden aktuell aus den Defaults der Klasse genommen.
        # Sie k√∂nnten auch √ºber argparse / config.yaml konfigurierbar gemacht werden.
    )

    # --- Lade Eingabe-CSV (Track) ---
    # ... (Laden und Pr√ºfen von track_df wie vorher) ...
    input_csv_path = args.input_csv
    base_filename = os.path.splitext(os.path.basename(input_csv_path))[0].replace("_track_data_full","")
    print(f"Verarbeite: {input_csv_path}")
    try: track_df = pd.read_csv(input_csv_path, parse_dates=['Time'], encoding='utf-8')
    except FileNotFoundError: print(f" Fehler: Eingabe-CSV nicht gefunden: {input_csv_path}"); sys.exit(1)
    except Exception as e: print(f" Fehler beim Lesen der CSV '{input_csv_path}': {e}"); sys.exit(1)
    required_cols = ['Distanz (km)', 'Elevation (m)', 'Time', 'Aufstieg (m)', 'Strecke Delta (km)', 'Latitude', 'Longitude']
    if not all(col in track_df.columns for col in required_cols): print(f" Fehler: Fehlende Spalten in {input_csv_path}. Ben√∂tigt: {required_cols}"); sys.exit(1)
    n_points = len(track_df)
    if n_points < max(20, config.smooth_window): # ... (handle too few points) ...
        print(f" Warnung: Zu wenig Punkte ({n_points})..."); open(args.output_plot, 'a').close(); open(args.output_peak_csv, 'a').close(); open(args.output_stats_csv, 'a').close(); print("[OK] Leere Ausgabedateien."); sys.exit(0)

    # --- Lade OPTIONALE Ortsdaten ---
    places_coords_df = None
    if args.places_coords_csv and os.path.exists(args.places_coords_csv):
         try:
             places_coords_df = pd.read_csv(args.places_coords_csv)
             if not all(c in places_coords_df.columns for c in ['Ort', 'Latitude_Center', 'Longitude_Center']):
                  print(f"[Warnung] Orts-CSV '{args.places_coords_csv}' fehlen Spalten. Ignoriere."); places_coords_df = None
             elif places_coords_df.empty: print(f"[Info] Orts-CSV '{args.places_coords_csv}' ist leer."); places_coords_df = None
             else: print(f"[Info] Ortsdaten f√ºr Annotation geladen: {args.places_coords_csv}")
         except Exception as e: print(f"[Warnung] Fehler beim Laden der Orts-CSV '{args.places_coords_csv}': {e}. Ignoriere."); places_coords_df = None
    else: print("[Info] Keine Ortsdaten-CSV f√ºr Annotation angegeben oder gefunden.")

    # --- Lade OPTIONALE POI-Daten f√ºr Wasserstellen ---
    water_pois_df = None
    if args.relevant_pois_csv and os.path.exists(args.relevant_pois_csv): # NEUES Argument
        try:
            all_relevant_pois_df = pd.read_csv(args.relevant_pois_csv)
            # Filtere nur 'drinking_water' POIs
            water_pois_df = all_relevant_pois_df[all_relevant_pois_df['Typ'].str.lower() == 'drinking_water'].copy()
            
            if not all(c in water_pois_df.columns for c in ['Name', 'Latitude', 'Longitude']): # Ben√∂tigte Spalten
                 print(f"[Warnung] Wasserstellen-relevante POI-CSV '{args.relevant_pois_csv}' fehlen Spalten 'Name', 'Latitude', 'Longitude'. Ignoriere Wasserstellen.")
                 water_pois_df = None
            elif water_pois_df.empty:
                print(f"[Info] Keine 'drinking_water' POIs in '{args.relevant_pois_csv}' gefunden.")
                water_pois_df = None
            else:
                print(f"[Info] Wasserstellen-POIs f√ºr Annotation geladen: {len(water_pois_df)} St√ºck.")
        except Exception as e:
            print(f"[Warnung] Fehler beim Laden der relevanten POI-CSV '{args.relevant_pois_csv}': {e}. Ignoriere Wasserstellen.")
            water_pois_df = None
    else:
        print("[Info] Keine relevante POI-CSV f√ºr Wasserstellen angegeben oder gefunden.")

    # --- 2. Gl√§tte H√∂henprofil ---
    track_df['Elevation_smooth (m)'] = smooth_elev(track_df['Elevation (m)'].values, config)
    elev_smooth = track_df['Elevation_smooth (m)'].values
    dist_m = track_df['Distanz (km)'].values * 1000.0
    track_len_km = dist_m[-1] / 1000.0

    # --- 3. Finde Peaks ---
    # ... (Peak-Findung wie vorher) ...
    peak_indices, properties = find_peaks(elev_smooth, prominence=config.min_peak_prominence_m)
    peaks_to_analyze_indices = []
    if len(peak_indices) > 0:
        peak_heights = elev_smooth[peak_indices]; sorted_peak_order = np.argsort(peak_heights)[::-1]
        sorted_peak_indices = peak_indices[sorted_peak_order]; peaks_to_analyze_indices = sorted_peak_indices[:2]
        print(f"  -> {len(peak_indices)} Peaks gefunden. Analysiere Top {len(peaks_to_analyze_indices)}:")
    else: print(f"  -> Keine signifikanten Peaks gefunden.")

    # --- 4. Analysiere Segmente ---
    # ... (Segment-Analyse wie vorher) ...
    all_segments_combined: List[Segment] = []; peak_data_for_csv: List[Dict] = []; excluded_indices_for_peak2: Set[int] = set()
    if len(peaks_to_analyze_indices) >= 1:
        peak1_idx = peaks_to_analyze_indices[0]; segments_p1, valid_indices_p1 = analyze_single_peak(peak1_idx, 1, elev_smooth, dist_m, track_len_km, n_points, config, None)
        all_segments_combined.extend(segments_p1); excluded_indices_for_peak2.update(valid_indices_p1)
        peak_data_for_csv.append({"item_type": "Peak", "peak_rank": 1,"peak_dist_km": dist_m[peak1_idx]/1000.0, "peak_elev_m": elev_smooth[peak1_idx]})
    if len(peaks_to_analyze_indices) >= 2:
        peak2_idx = peaks_to_analyze_indices[1]; print(f"  -> Analysiere Peak 2..."); segments_p2, _ = analyze_single_peak(peak2_idx, 2, elev_smooth, dist_m, track_len_km, n_points, config, excluded_indices_for_peak2)
        all_segments_combined.extend(segments_p2); peak_data_for_csv.append({"item_type": "Peak", "peak_rank": 2,"peak_dist_km": dist_m[peak2_idx]/1000.0, "peak_elev_m": elev_smooth[peak2_idx]})

    # --- 5. Berechne Gesamtstatistiken ---
    # ... (Statistik-Berechnung und Speichern wie vorher) ...
    print("[Info] Berechne Gesamtstatistiken...")
    stats_dict = calculate_statistics(track_df.copy(), config)
    stats_df = pd.DataFrame(list(stats_dict.items()), columns=["Statistik", "Wert"])
    try: output_dir = os.path.dirname(args.output_stats_csv); os.makedirs(output_dir, exist_ok=True); stats_df.to_csv(args.output_stats_csv, index=False, encoding='utf-8'); print(f"  -> Statistiken gespeichert: {args.output_stats_csv}")
    except Exception as e: print(f" Fehler beim Speichern der Statistiken '{args.output_stats_csv}': {e}")

    # --- 6. Bereite Peak/Segment Daten f√ºr CSV vor ---
    # ... (Peak/Segment-Daten speichern wie vorher) ...
    segment_data_for_csv: List[Dict] = []
    for seg in all_segments_combined:
        # Check validity using config threshold
        if seg.gain_m >= config.gain_threshold:
            # --- KORREKTE EINR√úCKUNG HIER ---
            notes = []
            # Check for start/end hints
            if seg.direction == "backward" and seg.start_idx == 0:
                notes.append("Starts@TrackBegin")
            if seg.direction == "forward" and seg.end_idx == n_points - 1: # n_points muss hier bekannt sein
                notes.append("Ends@TrackEnd")

            # Diese Zeile muss auf der gleichen Ebene wie notes=[] stehen
            segment_data_for_csv.append({
                "item_type": "Valid Segment",
                "peak_rank": seg.peak_rank,
                "segment_direction": seg.direction,
                "segment_start_km": dist_m[seg.start_idx] / 1000.0,
                "segment_end_km": dist_m[seg.end_idx] / 1000.0,
                "segment_length_m": seg.length_m,
                "segment_gain_m": seg.gain_m,
                "notes": "; ".join(notes)
            })
            # --- ENDE DES IF-BLOCKS ---

    # Nach der for-Schleife
    combined_data = peak_data_for_csv + segment_data_for_csv
    peak_segment_df = pd.DataFrame(combined_data)
    csv_cols = ["item_type", "peak_rank", "peak_dist_km", "peak_elev_m","segment_direction", "segment_start_km", "segment_end_km","segment_length_m", "segment_gain_m", "notes"]; peak_segment_df = peak_segment_df.reindex(columns=csv_cols) # Besser als manuelles hinzuf√ºgen
    try: output_dir = os.path.dirname(args.output_peak_csv); os.makedirs(output_dir, exist_ok=True); peak_segment_df.to_csv(args.output_peak_csv, index=False, encoding='utf-8', float_format='%.3f'); print(f"  -> Peak/Segment Daten gespeichert: {args.output_peak_csv}")
    except Exception as e: print(f" Fehler beim Speichern der Peak/Segment Daten '{args.output_peak_csv}': {e}")

    # --- 7. Erstelle Plot ---
    # Verwende den Original-Track df f√ºr Lat/Lon, aber mit gegl√§tteter H√∂he f√ºr die Y-Achse des Plots
    plot_df_for_plot = track_df[['Distanz (km)', 'Elevation_smooth (m)', 'Latitude', 'Longitude']].rename(columns={'Elevation_smooth (m)':'Elevation (m)'})
    plot_profile(base_filename, plot_df_for_plot,
                 list(peaks_to_analyze_indices),
                 all_segments_combined,
                 config,
                 args.output_plot,
                 places_coords_df,
                 water_pois_df)   

    print(f" Analyse abgeschlossen f√ºr: {base_filename}")

# ------------------------------------------------------------
#  Command Line Interface (ERWEITERT f√ºr optionalen Orts-Input)
# ------------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Analyze GPX track data, find peaks, calculate stats, generate plots, and optionally annotate with places, water sources.")
    parser.add_argument("--input-csv", required=True, help="Path to the input track data CSV file (from step 2).")
    parser.add_argument("--output-plot", required=True, help="Path to save the output plot PNG file.")
    parser.add_argument("--output-peak-csv", required=True, help="Path to save the peak and segment data CSV file.")
    parser.add_argument("--output-stats-csv", required=True, help="Path to save the overall statistics CSV file.")
    parser.add_argument("--places-coords-csv", help="Optional path to the CSV file with places and coordinates (output of step 8b).") # Optionaler Input
    parser.add_argument("--relevant-pois-csv", help="Optional path to the CSV file with relevant POIs (e.g., for water sources).")     
    # Config arguments
    parser.add_argument("--smooth-window", type=int, default=11); parser.add_argument("--smooth-poly", type=int, default=2)
    parser.add_argument("--gain-threshold", type=float, default=30.0); parser.add_argument("--eps-height", type=float, default=0.3)
    parser.add_argument("--prominence", type=float, default=40.0); parser.add_argument("--peak-edge-km", type=float, default=0.25)
    parser.add_argument("--plot-dpi", type=int, default=150); parser.add_argument("--plot-x-tick-major", type=float, default=5.0); parser.add_argument("--plot-x-tick-minor", type=float, default=1.0)
    parser.add_argument("--pause-min-duration", type=float, default=120.0); parser.add_argument("--pause-max-distance", type=float, default=5.0)
    # TODO: Optional: Argumente f√ºr die Offset-Bins/Werte hinzuf√ºgen

    args = parser.parse_args()
    main(args)