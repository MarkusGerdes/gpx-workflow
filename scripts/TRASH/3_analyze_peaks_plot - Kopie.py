#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
3_analyze_peaks_plot.py
-----------------------
Analyzes track data from a CSV file (generated by 2_parse_gpx_full.py)
to find significant peaks and their associated segments. Calculates
overall and time-based statistics. Generates a detailed elevation profile plot
with slope coloring AND optional place annotations. Saves results to separate files.

Requires: pandas, numpy, matplotlib, scipy, geopy, typing
"""

import os
import csv
import argparse
from dataclasses import dataclass, field
from typing import List, Tuple, Optional, Set, Dict # Ensure Optional is imported
import time # For pause calculation

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from matplotlib.collections import LineCollection
from matplotlib.colors import ListedColormap, BoundaryNorm
from matplotlib.patches import Patch
import numpy as np
import pandas as pd
from scipy.signal import savgol_filter, find_peaks
from geopy.distance import distance as geopy_distance # for pause detection distance
from scipy.spatial import KDTree # For nearest neighbor search

# ------------------------------------------------------------
#  Konfigurationsobjekt (wird jetzt aus Argumenten befüllt)
# ------------------------------------------------------------
@dataclass
class Config:
    """Konfigurationsparameter für die Analyse (aus Argumenten)."""
    # Glättung
    smooth_window: int = 11
    smooth_poly: int = 2
    # Segmenterkennung
    gain_threshold: float = 30.0
    eps_height: float = 0.3
    # Peak-Erkennung
    min_peak_prominence_m: float = 40.0
    peak_edge_km: float = 0.25
    # Plot Einstellungen
    plot_dpi: int = 150
    plot_x_tick_major: float = 5.0
    plot_x_tick_minor: float = 1.0
    min_length_draw_m: float = 100.0
    # Farben/Alpha (Hardcoded oder später aus config.yaml lesen)
    color_fwd_valid_label: str = "#1f77b4"
    color_bwd_valid_label: str = "#2ca02c"
    color_peak1: str = "#ff7f0e"
    color_peak2: str = "#9467bd"
    color_fwd_shade: str = "#d6ebf2"
    color_bwd_shade: str = "#d8f0d3"
    shade_alpha: float = 0.4
    color_invalid: str = "#cccccc"
    invalid_alpha: float = 0.20
    # Steigungsfarben/-schwellen
    slope_thresholds: List[float] = field(default_factory=lambda: [-100, -12, -8, -5, -2, -0.5, 0.5, 2, 5, 8, 12, 100])
    slope_colors: List[str] = field(default_factory=lambda: [
        '#08306b', '#08519c', '#3182bd', '#9ecae1', '#deebf7', # Blues
        '#d9d9d9', # Grey
        '#e5f5e0', '#a1d99b', '#fed976', '#fd8d3c', '#e31a1c', # Greens/Yellows/Oranges/Red
    ])
    slope_labels: List[str] = field(default_factory=lambda: [
        "<-12%", "-12..-8%", "-8..-5%", "-5..-2%", "-2..-0.5%",
        "Flat",
        "0.5..2%", "2..5%", "5..8%", "8..12%", ">12%"
    ])
    slope_linewidth: float = 2.0
    # Pause Detection
    pause_min_duration_s: float = 120.0 # Min duration for a stop to be a pause
    pause_max_distance_m: float = 5.0   # Max distance moved during a pause
    # NEU: Ort Annotation
    place_marker_style: str = '^'
    place_marker_color: str = 'darkviolet'
    place_marker_size: int = 8
    place_text_color: str = 'darkviolet'
    place_text_size: int = 7
    place_text_offset_y: int = 12 # Vertical offset for text label above marker
    place_text_bg_alpha: float = 0.7 # Background alpha for text bbox

# ------------------------------------------------------------
#  Datenklasse Segment (unverändert)
# ------------------------------------------------------------
@dataclass
class Segment:
    peak_rank: int
    start_idx: int
    end_idx: int
    gain_m: float
    length_m: float
    direction: str # 'forward' or 'backward'

    @property
    def valid(self) -> bool:
        # IMPORTANT: Access threshold via the *instance* of Config passed around
        # Assuming the Config object is available in the scope where this is checked
        # This might need adjustment depending on how Config is passed.
        # For now, assuming a global or passed config object.
        # A better way would be to pass config to the Segment or check externally.
        # Let's assume it's checked externally for now, or Config needs to be global/passed.
        # Example if passed: def valid(self, config: Config) -> bool: ...
        # Hardcoding for now, needs refinement if Config isn't easily accessible here.
        return self.gain_m >= 30.0 # Fallback to hardcoded value

    def get_index_range(self) -> Set[int]:
        return set(range(self.start_idx, self.end_idx + 1))

# ------------------------------------------------------------
#  Helfer (angepasst für Config-Instanz)
# ------------------------------------------------------------
# smooth_elev (unverändert)
def smooth_elev(elev: np.ndarray, config: Config) -> np.ndarray:
    if len(elev) < config.smooth_window: return elev.copy()
    # Ensure window is odd and smaller than array length
    window = config.smooth_window if config.smooth_window % 2 != 0 else config.smooth_window + 1
    if window > len(elev):
        window = len(elev) if len(elev) % 2 != 0 else len(elev) - 1
        if window < 3: return elev.copy() # Need at least 3 points for poly order 2
    poly = config.smooth_poly
    # Ensure poly order is less than window size
    if poly >= window:
        poly = window - 1
        if poly < 1: poly = 1 # Minimum poly order is 1
    try:
        return savgol_filter(elev, window, poly)
    except ValueError as e:
        print(f"[Warnung] Fehler bei Savgol-Filter (window={window}, poly={poly}, len={len(elev)}): {e}. Ungeglättete Daten werden verwendet.")
        return elev.copy()

# ------------------------------------------------------------
#  Analyse‑Routinen (angepasst für Config-Instanz)
# ------------------------------------------------------------
# analyse_direction (unverändert)
def analyse_direction(
    elev: np.ndarray, dist: np.ndarray, peak_idx: int, direction: str,
    peak_rank: int, n: int, config: Config, excluded_indices: Optional[Set[int]] = None
) -> Tuple[List[Segment], List[Segment]]:
    """Analyzes segments in one direction from a peak."""
    if not (0 <= peak_idx < n): return [], []
    excluded_indices = excluded_indices or set()
    step = 1 if direction == "forward" else -1
    i = peak_idx + step

    mode = "falling" # Start assuming we are falling from the peak
    current_gain = 0.0
    segment_start_idx = -1 # Index where the current potential rising segment started

    all_found: List[Segment] = []
    first_valid: List[Segment] = []

    while 0 <= i < n:
        if i in excluded_indices:
            # If we hit an excluded index while rising, reset the segment
            if mode == "rising":
                mode = "falling"
                current_gain = 0.0
                segment_start_idx = -1
            i += step
            continue

        # Get previous index, ensure it's valid and not excluded
        prev_idx = i - step
        if not (0 <= prev_idx < n): break # Should not happen if loop condition is correct
        if prev_idx in excluded_indices:
            # If the previous point was excluded, we can't calculate diff, reset if rising
            if mode == "rising":
                 mode = "falling"
                 current_gain = 0.0
                 segment_start_idx = -1
            i += step
            continue

        # Calculate elevation difference
        diff = elev[i] - elev[prev_idx]

        # Ignore negligible height changes
        if abs(diff) < config.eps_height:
            i += step
            continue

        if mode == "falling":
            # If we start rising, mark the beginning of a potential segment
            if diff > 0:
                mode = "rising"
                segment_start_idx = prev_idx
                current_gain = diff
        else: # mode == "rising"
            # Continue accumulating gain if still rising
            if diff > 0:
                current_gain += diff
            # If we start falling, the rising segment ends
            else:
                segment_end_idx = prev_idx
                # Only record if a valid start was found
                if segment_start_idx != -1:
                    length_m = abs(dist[segment_end_idx] - dist[segment_start_idx])
                    # Ensure indices are ordered correctly for the Segment object
                    idx1, idx2 = min(segment_start_idx, segment_end_idx), max(segment_start_idx, segment_end_idx)
                    # Pass config.gain_threshold to check validity explicitly if needed
                    # For now, rely on the property check later
                    seg = Segment(peak_rank, idx1, idx2, current_gain, length_m, direction)
                    all_found.append(seg)
                    # Check validity using the threshold from config
                    if seg.gain_m >= config.gain_threshold and not first_valid:
                        first_valid.append(seg)
                        # break # Optional: Stop after finding the first valid segment
                # Reset for the next potential segment
                mode = "falling"
                current_gain = 0.0
                segment_start_idx = -1
        i += step

    # Handle case where a rising segment goes to the very end/start of the track
    if mode == "rising" and segment_start_idx != -1:
         segment_end_idx = i - step # The last valid index processed
         length_m = abs(dist[segment_end_idx] - dist[segment_start_idx])
         idx1, idx2 = min(segment_start_idx, segment_end_idx), max(segment_start_idx, segment_end_idx)
         seg = Segment(peak_rank, idx1, idx2, current_gain, length_m, direction)
         all_found.append(seg)
         if seg.gain_m >= config.gain_threshold and not first_valid:
             first_valid.append(seg)


    return all_found, first_valid

# analyze_single_peak (unverändert)
def analyze_single_peak(
    peak_idx: int, peak_rank: int, elev: np.ndarray, dist: np.ndarray,
    track_len_km: float, n: int, config: Config, excluded_indices: Optional[Set[int]] = None
) -> Tuple[List[Segment], Set[int]]:
    """Analyzes segments forward and backward from a single peak."""
    peak_dist_km = dist[peak_idx] / 1000.0
    peak_elev_m = elev[peak_idx]
    segments: List[Segment] = []
    valid_segment_indices: Set[int] = set()

    print(f"  Analysiere Peak {peak_rank} ({peak_elev_m:.1f} m @ {peak_dist_km:.2f} km):")

    # Check if peak is too close to the edges
    is_peak_near_start = peak_dist_km <= config.peak_edge_km
    is_peak_near_end = (track_len_km - peak_dist_km) <= config.peak_edge_km

    # --- Forward Analysis ---
    if is_peak_near_end:
        print(f"    -> P{peak_rank} am Ende - keine Vorwärtsanalyse.")
    else:
        all_fwd, valid_fwd = analyse_direction(elev, dist, peak_idx, "forward", peak_rank, n, config, excluded_indices)
        segments.extend(all_fwd)
        # Check validity explicitly using config
        valid_fwd_filtered = [s for s in valid_fwd if s.gain_m >= config.gain_threshold]
        if valid_fwd_filtered:
            s = valid_fwd_filtered[0]
            end_hint = " (-> Ende)" if s.end_idx == n - 1 else ""
            print(f"    -> OK P{peak_rank} Vorwärts: +{s.gain_m:.1f} m / {s.length_m:.0f} m{end_hint}")
            valid_segment_indices.update(s.get_index_range())
        else:
            print(f"    -> !! P{peak_rank} Vorwärts: Kein signif. Segment.")

    # --- Backward Analysis ---
    if is_peak_near_start:
        print(f"    <- P{peak_rank} am Start - keine Rückwärtsanalyse.")
    else:
        all_bwd, valid_bwd = analyse_direction(elev, dist, peak_idx, "backward", peak_rank, n, config, excluded_indices)
        segments.extend(all_bwd)
        # Check validity explicitly using config
        valid_bwd_filtered = [s for s in valid_bwd if s.gain_m >= config.gain_threshold]
        if valid_bwd_filtered:
            s = valid_bwd_filtered[0]
            start_hint = " (<- Start)" if s.start_idx == 0 else ""
            print(f"    <- OK P{peak_rank} Rückwärts: +{s.gain_m:.1f} m / {s.length_m:.0f} m{start_hint}")
            valid_segment_indices.update(s.get_index_range())
        else:
            print(f"    <- !! P{peak_rank} Rückwärts: Kein signif. Segment.")

    return segments, valid_segment_indices


# ------------------------------------------------------------
#  Statistik-Berechnung (angepasst für Config Instanz bei Pausen)
# ------------------------------------------------------------
def calculate_statistics(df: pd.DataFrame, config: Config) -> Dict[str, float or str]:
    """Calculates overall and time-based statistics from the track DataFrame."""
    stats = {}
    n_points = len(df)
    if n_points < 2: return {"Error": "Not enough data points"}

    # Basic Stats
    stats["Gesamtdistanz (km)"] = df["Distanz (km)"].iloc[-1]
    stats["Minimalhöhe (m)"] = df["Elevation (m)"].min()
    stats["Maximalhöhe (m)"] = df["Elevation (m)"].max()
    stats["Gesamter Aufstieg (m)"] = df["Aufstieg (m)"].sum() # Assumes 'Aufstieg (m)' column exists
    elevation_diff = df['Elevation (m)'].diff().fillna(0)
    stats["Gesamter Abstieg (m)"] = abs(elevation_diff.clip(upper=0).sum())

    # Time Stats
    start_time = df['Time'].iloc[0]
    end_time = df['Time'].iloc[-1]
    total_duration_td = end_time - start_time
    stats["Gesamtdauer"] = str(total_duration_td).split('.')[0]

    pause_duration_s = 0.0
    df['TimeDelta (s)'] = df['Time'].diff().dt.total_seconds().fillna(0)
    df['DistDelta (m)'] = df['Strecke Delta (km)'] * 1000

    for i in range(1, n_points):
         time_diff = df.loc[i, 'TimeDelta (s)']
         dist_diff = df.loc[i, 'DistDelta (m)']
         # Use pause parameters from config object
         if time_diff >= config.pause_min_duration_s and dist_diff <= config.pause_max_distance_m:
              pause_duration_s += time_diff

    moving_duration_s = total_duration_td.total_seconds() - pause_duration_s
    moving_duration_td = pd.to_timedelta(moving_duration_s, unit='s')
    pause_duration_td = pd.to_timedelta(pause_duration_s, unit='s')

    stats["Pausenzeit"] = str(pause_duration_td).split('.')[0]
    stats["Bewegungszeit"] = str(moving_duration_td).split('.')[0]

    total_duration_h = total_duration_td.total_seconds() / 3600
    moving_duration_h = moving_duration_td.total_seconds() / 3600

    stats["Ø Geschwindigkeit (km/h)"] = (stats["Gesamtdistanz (km)"] / total_duration_h) if total_duration_h > 0 else 0.0
    stats["Ø Geschw. in Bewegung (km/h)"] = (stats["Gesamtdistanz (km)"] / moving_duration_h) if moving_duration_h > 0 else 0.0

    df.drop(columns=['TimeDelta (s)', 'DistDelta (m)'], inplace=True, errors='ignore')

    for key, value in stats.items():
        if isinstance(value, (float, np.float64)):
            stats[key] = f"{value:.2f}"

    return stats


# ------------------------------------------------------------
#  Plot‑Funktionen (ERWEITERT für Orte)
# ------------------------------------------------------------
# _calculate_slope_colors (unverändert)
def _calculate_slope_colors(dist_m: np.ndarray, elev_m: np.ndarray, config: Config) -> Tuple[np.ndarray, ListedColormap, BoundaryNorm]:
    """Berechnet Steigungsprozente und weist Farben basierend auf Schwellen zu."""
    if len(dist_m) < 2: return np.array([]), ListedColormap([]), BoundaryNorm([], 0)

    d_elev = np.gradient(elev_m)
    d_dist = np.gradient(dist_m)

    slope_percent = np.zeros_like(d_dist)
    min_dist_step = 1e-1
    valid_dist_mask = d_dist > min_dist_step

    slope_percent[valid_dist_mask] = (d_elev[valid_dist_mask] / d_dist[valid_dist_mask]) * 100
    slope_percent = np.clip(slope_percent, config.slope_thresholds[0], config.slope_thresholds[-1])

    cmap = ListedColormap(config.slope_colors)
    norm = BoundaryNorm(config.slope_thresholds, cmap.N)
    slope_indices = np.digitize(slope_percent[:-1], config.slope_thresholds[1:], right=False)

    return slope_indices, cmap, norm

# _shade_segment (unverändert)
def _shade_segment(ax, dist_km: np.ndarray, elev_m: np.ndarray, seg: Segment, config: Config):
    """Zeichnet Hintergrundschattierungen und Labels für Segmente."""
    # Use config.gain_threshold to check validity
    is_valid = seg.gain_m >= config.gain_threshold

    if is_valid:
        shade_color = config.color_fwd_shade if seg.direction == "forward" else config.color_bwd_shade
        shade_alpha = config.shade_alpha
        text_color = config.color_fwd_valid_label if seg.direction == "forward" else config.color_bwd_valid_label
    else: # Invalid segment
        shade_color = config.color_invalid
        shade_alpha = config.invalid_alpha
        text_color = "#666666" # Muted color for invalid labels if shown

    x0_km, x1_km = dist_km[seg.start_idx], dist_km[seg.end_idx]
    segment_width_km = abs(x1_km - x0_km)

    # Draw shading if valid or long enough invalid
    if is_valid or seg.length_m >= config.min_length_draw_m:
         ax.axvspan(x0_km, x1_km, color=shade_color, alpha=shade_alpha, zorder=1, lw=0)

         # Draw text label ONLY for VALID segments and if segment is wide enough on plot
         if is_valid and segment_width_km > 0.5: # Only label if wider than 0.5km on plot
            text_x_km = (x0_km + x1_km) / 2
            seg_indices = range(seg.start_idx, seg.end_idx + 1)
            # --- KORREKTUR HIER ---
            if not list(seg_indices): return # Use return to exit function if range is empty
            # ----------------------
            text_y_max_in_segment = np.max(elev_m[list(seg_indices)]) # Ensure indices are list for numpy < 1.25
            y_offset = 5
            ax.text(text_x_km, text_y_max_in_segment + y_offset, f"P{seg.peak_rank}:{seg.gain_m:.0f}m",
                    ha="center", va="bottom", fontsize=8,
                    color=text_color, weight='bold',
                    bbox=dict(boxstyle="round,pad=0.15", fc="white", ec="none", alpha=0.75),
                    zorder=5)

# NEU: Funktion zum Finden des nächsten Punktes (aus Skript 9b übernommen)
def find_nearest_track_point_kdtree(track_lat_lon: np.ndarray, place_lat: float, place_lon: float) -> int:
    """Finds the index of the nearest track point using KDTree."""
    if track_lat_lon is None or track_lat_lon.shape[0] == 0: return -1
    try:
        tree = KDTree(track_lat_lon)
        distance, index = tree.query([place_lat, place_lon])
        return index
    except Exception as e:
        print(f"[Warnung] KDTree Fehler: {e}")
        return -1

# plot_profile (ERWEITERT um optionale Ortsdaten)
def plot_profile(base_filename: str,
                 track_df: pd.DataFrame, # Übergebe ganzen DataFrame
                 peak_indices: List[int],
                 all_segments: List[Segment],
                 config: Config,
                 output_plot_path: str,
                 places_coords_df: Optional[pd.DataFrame] = None): # Optionaler DataFrame für Orte
    """Erstellt und speichert den detaillierten Höhenprofil-Plot, optional mit Ortsmarkern."""
    print(f"[Info] Erstelle Plot: {output_plot_path}")
    fig, ax = plt.subplots(figsize=(14, 7.5))

    # Extrahiere Daten aus DataFrame
    dist_m = track_df['Distanz (km)'].values * 1000.0
    elev_m = track_df['Elevation (m)'].values # Verwende geglättete Höhe für Peaks/Segmente, aber rohe für Plot? Nein, geglättet ist besser für Steigung.
    dist_km = dist_m / 1000.0
    n_points = len(dist_m)

    if n_points < 2:
        print("[Warnung] Weniger als 2 Punkte für Plot vorhanden.")
        plt.close(fig)
        try: open(output_plot_path, 'a').close()
        except: pass
        return

    # 1. Calculate slope colors (using smoothed elevation for consistency with peak analysis)
    slope_indices, cmap, norm = _calculate_slope_colors(dist_m, elev_m, config)

    if len(slope_indices) == 0:
         ax.plot(dist_km, elev_m, color='black', linewidth=1.5, zorder=2, label='Höhenprofil (Fallback)')
    else:
        # 2. Create LineCollection
        points = np.array([dist_km, elev_m]).T.reshape(-1, 1, 2)
        segments_lc = np.concatenate([points[:-1], points[1:]], axis=1)
        lc = LineCollection(segments_lc, cmap=cmap, norm=norm, linewidth=config.slope_linewidth, zorder=2)
        lc.set_array(slope_indices)
        line = ax.add_collection(lc)
        ax.set_xlim(dist_km.min(), dist_km.max())
        ax.set_ylim(elev_m.min() - 10, elev_m.max() + 30) # Mehr Platz oben für Labels

    # 3. Mark and label peaks
    peak_colors = [config.color_peak1, config.color_peak2]
    peak_handles = []
    peak_labels_text = []
    for i, peak_idx in enumerate(peak_indices):
        if i >= len(peak_colors): break
        peak_x_km, peak_y_m = dist_km[peak_idx], elev_m[peak_idx]
        color = peak_colors[i]; label = f"Peak {i+1}"
        peak_labels_text.append(label)
        handle = plt.Line2D([0], [0], color=color, linestyle='--', linewidth=1.5, label=label)
        peak_handles.append(handle)
        ax.axvline(peak_x_km, color=color, linestyle="--", linewidth=1.5, zorder=3, alpha=0.8)
        ax.plot(peak_x_km, peak_y_m, 'o', color=color, markersize=7, zorder=4, mec='black', mew=0.5)
        ax.text(peak_x_km + 0.15 * (ax.get_xlim()[1] - ax.get_xlim()[0])/100 ,
                peak_y_m, f"{peak_y_m:.0f} m",
                color='black', fontsize=8, va='center', ha='left',
                bbox=dict(boxstyle="round,pad=0.15", fc=color, ec="black", lw=0.5, alpha=0.85),
                zorder=6) # Höherer zorder

    # 4. Shade ascent segments
    for seg in all_segments:
        # Check validity using the config threshold
        is_valid = seg.gain_m >= config.gain_threshold
        _shade_segment(ax, dist_km, elev_m, seg, config) # Übergibt jetzt valides Segment

    # ***** NEU: 5. Orte annotieren (wenn Daten vorhanden) *****
    place_annotations = []
    if places_coords_df is not None and not places_coords_df.empty:
        print("[Info] Füge Ortsmarker zum Plot hinzu...")
        # Bereite Track-Koordinaten für KDTree vor
        track_coords_latlon = track_df[['Latitude', 'Longitude']].values
        if track_coords_latlon.shape[0] > 0: # Nur wenn Trackpunkte existieren
            for _, place_row in places_coords_df.iterrows():
                place_name = place_row['Ort']
                place_lat = place_row.get('Latitude_Center')
                place_lon = place_row.get('Longitude_Center')

                if pd.notna(place_lat) and pd.notna(place_lon):
                    nearest_idx = find_nearest_track_point_kdtree(track_coords_latlon, place_lat, place_lon)
                    if nearest_idx != -1:
                        # Extrahiere Distanz und Höhe des nächstgelegenen Routenpunktes
                        track_point = track_df.iloc[nearest_idx]
                        plot_dist_km = track_point['Distanz (km)']
                        plot_elev_m = track_point['Elevation (m)'] # Höhe des Routenpunktes
                        place_annotations.append({
                            'name': place_name, 'x': plot_dist_km, 'y': plot_elev_m
                        })
                    else:
                         print(f"[Warnung] Konnte nächsten Punkt für Ort '{place_name}' nicht finden.")
                else:
                     print(f"[Warnung] Fehlende Koordinaten für Ort '{place_name}'. Überspringe Annotation.")

            # Zeichne die Marker und Texte für Orte
            for anno in place_annotations:
                ax.plot(anno['x'], anno['y'],
                        marker=config.place_marker_style,
                        color=config.place_marker_color,
                        markersize=config.place_marker_size,
                        linestyle='None', # Kein Linienstil für Marker
                        zorder=5) # Unter Peak-Labels aber über Segmenten
                ax.text(anno['x'], anno['y'] + config.place_text_offset_y, anno['name'],
                        color=config.place_text_color,
                        fontsize=config.place_text_size,
                        ha='center', va='bottom',
                        bbox=dict(boxstyle="round,pad=0.15", fc="white", ec=config.place_marker_color, lw=0.5, alpha=config.place_text_bg_alpha),
                        zorder=6) # Über Markern
        else:
             print("[Warnung] Keine Track-Koordinaten für KDTree verfügbar.")


    # 6. Achsen, Ticks, Titel, Grid setup (vorher 5)
    ax.set_xlabel("Distanz (km)")
    ax.set_ylabel("Höhe (m)")
    ax.set_title(f"Analyse (Steigung, 2 Peaks{' & Orte' if place_annotations else ''}) – {base_filename}", pad=15) # Titel anpassen
    ax.grid(True, which='major', linestyle='-', linewidth='0.4', color='lightgray', zorder=0)
    ax.grid(True, which='minor', linestyle=':', linewidth='0.3', color='#ebebeb', zorder=0)
    ax.xaxis.set_major_locator(ticker.MultipleLocator(config.plot_x_tick_major))
    ax.xaxis.set_minor_locator(ticker.MultipleLocator(config.plot_x_tick_minor))
    ax.yaxis.set_minor_locator(ticker.AutoMinorLocator())
    ax.tick_params(axis='both', which='major', labelsize=9)

    # 7. Create legend below the plot (vorher 6)
    slope_handles = [Patch(color=color, label=label) for color, label in zip(config.slope_colors, config.slope_labels)]
    # Füge optional einen Handle für Orte hinzu, wenn welche geplottet wurden
    place_handle = []
    if place_annotations:
         place_handle = [plt.Line2D([0], [0], marker=config.place_marker_style, color=config.place_marker_color, label='Ort', linestyle='None', markersize=config.place_marker_size)]

    all_handles = peak_handles + place_handle + slope_handles
    all_labels = peak_labels_text + (['Ort'] if place_annotations else []) + config.slope_labels

    num_legend_items = len(all_handles)
    ncol = 6 if num_legend_items > 12 else 5 if num_legend_items > 8 else 4
    fig.legend(handles=all_handles, labels=all_labels,
               loc='lower center',
               bbox_to_anchor=(0.5, 0.01),
               ncol=ncol,
               fontsize=8, title="Legende", title_fontsize=9)

    fig.tight_layout(rect=[0, 0.08, 1, 0.96])

    # 8. Save the plot (vorher 7)
    try:
        output_dir = os.path.dirname(output_plot_path)
        if output_dir:
             os.makedirs(output_dir, exist_ok=True)
        fig.savefig(output_plot_path, dpi=config.plot_dpi, bbox_inches='tight')
        print(f"  -> Plot gespeichert: '{output_plot_path}'") # Angepasste Ausgabe
    except Exception as e:
        print(f"  -> Fehler beim Speichern des Plots '{output_plot_path}': {e}")
    finally:
        plt.close(fig)

# ------------------------------------------------------------
#  Haupt‑Routine (ERWEITERT für optionalen Orts-Input)
# ------------------------------------------------------------
def main(args):
    """Hauptfunktion: Verarbeitet eine CSV-Datei, erstellt Plot und Daten-CSVs."""
    print("Starte Peak/Profil Analyse V4 (mit optionalen Orten)...")

    # --- Lade Konfiguration aus Argumenten ---
    config = Config(
        smooth_window=args.smooth_window,
        smooth_poly=args.smooth_poly,
        gain_threshold=args.gain_threshold,
        eps_height=args.eps_height,
        min_peak_prominence_m=args.prominence,
        peak_edge_km=args.peak_edge_km,
        plot_dpi=args.plot_dpi,
        plot_x_tick_major=args.plot_x_tick_major,
        plot_x_tick_minor=args.plot_x_tick_minor,
        pause_min_duration_s=args.pause_min_duration,
        pause_max_distance_m=args.pause_max_distance
    )

    # --- Lade Eingabe-CSV ---
    input_csv_path = args.input_csv
    base_filename = os.path.splitext(os.path.basename(input_csv_path))[0].replace("_track_data_full","")
    print(f"Verarbeite: {input_csv_path}")
    try:
        track_df = pd.read_csv(input_csv_path, parse_dates=['Time'], encoding='utf-8')
    except FileNotFoundError:
        print(f" Fehler: Eingabe-CSV nicht gefunden: {input_csv_path}")
        sys.exit(1)
    except Exception as e:
        print(f" Fehler beim Lesen der CSV '{input_csv_path}': {e}")
        sys.exit(1)

    required_cols = ['Distanz (km)', 'Elevation (m)', 'Time', 'Aufstieg (m)', 'Strecke Delta (km)', 'Latitude', 'Longitude']
    if not all(col in track_df.columns for col in required_cols):
        print(f" Fehler: Fehlende Spalten in {input_csv_path}. Benötigt: {required_cols}")
        sys.exit(1)

    n_points = len(track_df)
    if n_points < max(20, config.smooth_window):
        print(f" Warnung: Zu wenig Punkte ({n_points}) in {input_csv_path} für die Analyse.")
        open(args.output_plot, 'a').close()
        open(args.output_peak_csv, 'a').close()
        open(args.output_stats_csv, 'a').close()
        print("[OK] Leere Ausgabedateien erstellt.")
        sys.exit(0)

    # --- Lade OPTIONALE Ortsdaten ---
    places_coords_df = None
    if args.places_coords_csv and os.path.exists(args.places_coords_csv):
         try:
             places_coords_df = pd.read_csv(args.places_coords_csv)
             # Check required columns
             if not all(c in places_coords_df.columns for c in ['Ort', 'Latitude_Center', 'Longitude_Center']):
                  print(f"[Warnung] Orts-CSV '{args.places_coords_csv}' fehlen Spalten. Annotationen werden ignoriert.")
                  places_coords_df = None
             elif places_coords_df.empty:
                  print(f"[Info] Orts-CSV '{args.places_coords_csv}' ist leer.")
                  places_coords_df = None # Treat empty as None
             else:
                  print(f"[Info] Ortsdaten für Annotation geladen: {args.places_coords_csv}")
         except Exception as e:
              print(f"[Warnung] Fehler beim Laden der Orts-CSV '{args.places_coords_csv}': {e}. Annotationen werden ignoriert.")
              places_coords_df = None
    else:
         print("[Info] Keine Ortsdaten-CSV für Annotation angegeben oder gefunden.")


    # --- 2. Glätte Höhenprofil ---
    # Wichtig: Glätte die Elevation direkt im DataFrame für Konsistenz
    track_df['Elevation_smooth (m)'] = smooth_elev(track_df['Elevation (m)'].values, config)
    elev_smooth = track_df['Elevation_smooth (m)'].values # Verwende diese für Analyse & Plot
    dist_m = track_df['Distanz (km)'].values * 1000.0
    track_len_km = dist_m[-1] / 1000.0

    # --- 3. Finde Peaks (auf geglätteten Daten) ---
    peak_indices, properties = find_peaks(elev_smooth, prominence=config.min_peak_prominence_m)
    if len(peak_indices) == 0:
        print(f"  -> Keine signifikanten Peaks gefunden.")
        peaks_to_analyze_indices = []
        all_segments_combined = []
    else:
        peak_heights = elev_smooth[peak_indices]
        sorted_peak_order = np.argsort(peak_heights)[::-1]
        sorted_peak_indices = peak_indices[sorted_peak_order]
        peaks_to_analyze_indices = sorted_peak_indices[:2]
        print(f"  -> {len(peak_indices)} Peaks gefunden. Analysiere Top {len(peaks_to_analyze_indices)}:")

    # --- 4. Analysiere Segmente (mit geglätteten Daten) ---
    all_segments_combined: List[Segment] = []
    peak_data_for_csv: List[Dict] = []
    excluded_indices_for_peak2: Set[int] = set()

    if len(peaks_to_analyze_indices) >= 1:
        peak1_idx = peaks_to_analyze_indices[0]
        segments_p1, valid_indices_p1 = analyze_single_peak(
            peak1_idx, 1, elev_smooth, dist_m, track_len_km, n_points, config, None)
        all_segments_combined.extend(segments_p1)
        excluded_indices_for_peak2.update(valid_indices_p1)
        peak_data_for_csv.append({
             "item_type": "Peak", "peak_rank": 1,
             "peak_dist_km": dist_m[peak1_idx]/1000.0, "peak_elev_m": elev_smooth[peak1_idx]
        })

    if len(peaks_to_analyze_indices) >= 2:
        peak2_idx = peaks_to_analyze_indices[1]
        print(f"  -> Analysiere Peak 2 (unter Ausschluss valider P1-Segmente)...")
        segments_p2, _ = analyze_single_peak(
            peak2_idx, 2, elev_smooth, dist_m, track_len_km, n_points, config, excluded_indices_for_peak2)
        all_segments_combined.extend(segments_p2)
        peak_data_for_csv.append({
            "item_type": "Peak", "peak_rank": 2,
            "peak_dist_km": dist_m[peak2_idx]/1000.0, "peak_elev_m": elev_smooth[peak2_idx]
        })

    # --- 5. Berechne Gesamtstatistiken (nutzt Original-DataFrame df) ---
    print("[Info] Berechne Gesamtstatistiken...")
    stats_dict = calculate_statistics(track_df.copy(), config) # Pass a copy to avoid modifying original df in function
    stats_df = pd.DataFrame(list(stats_dict.items()), columns=["Statistik", "Wert"])
    try:
        output_dir = os.path.dirname(args.output_stats_csv)
        if output_dir: os.makedirs(output_dir, exist_ok=True)
        stats_df.to_csv(args.output_stats_csv, index=False, encoding='utf-8')
        print(f"  -> Statistiken gespeichert: {args.output_stats_csv}")
    except Exception as e:
        print(f" Fehler beim Speichern der Statistiken '{args.output_stats_csv}': {e}")


    # --- 6. Bereite Peak/Segment Daten für CSV vor ---
    segment_data_for_csv: List[Dict] = []
    for seg in all_segments_combined:
         # Check validity using config threshold
        if seg.gain_m >= config.gain_threshold:
            notes = []
            if seg.direction == "backward" and seg.start_idx == 0: notes.append("Starts@TrackBegin")
            if seg.direction == "forward" and seg.end_idx == n_points - 1: notes.append("Ends@TrackEnd")
            segment_data_for_csv.append({
                "item_type": "Valid Segment", "peak_rank": seg.peak_rank,
                "segment_direction": seg.direction,
                "segment_start_km": dist_m[seg.start_idx] / 1000.0,
                "segment_end_km": dist_m[seg.end_idx] / 1000.0,
                "segment_length_m": seg.length_m,
                "segment_gain_m": seg.gain_m,
                "notes": "; ".join(notes)
            })
    combined_data = peak_data_for_csv + segment_data_for_csv
    peak_segment_df = pd.DataFrame(combined_data)
    csv_cols = [ # Define explicit order
        "item_type", "peak_rank", "peak_dist_km", "peak_elev_m",
        "segment_direction", "segment_start_km", "segment_end_km",
        "segment_length_m", "segment_gain_m", "notes" ]
    for col in csv_cols: # Ensure all columns exist
        if col not in peak_segment_df.columns: peak_segment_df[col] = pd.NA
    peak_segment_df = peak_segment_df[csv_cols] # Reorder
    try:
        output_dir = os.path.dirname(args.output_peak_csv)
        if output_dir: os.makedirs(output_dir, exist_ok=True)
        peak_segment_df.to_csv(args.output_peak_csv, index=False, encoding='utf-8', float_format='%.3f')
        print(f"  -> Peak/Segment Daten gespeichert: {args.output_peak_csv}")
    except Exception as e:
        print(f" Fehler beim Speichern der Peak/Segment Daten '{args.output_peak_csv}': {e}")


    # --- 7. Erstelle Plot (mit optionalen Orten) ---
    # Übergebe den DataFrame mit der geglätteten Höhe für den Plot
    plot_profile(base_filename, track_df[['Distanz (km)', 'Elevation_smooth (m)', 'Latitude', 'Longitude']].rename(columns={'Elevation_smooth (m)':'Elevation (m)'}),
                 list(peaks_to_analyze_indices),
                 all_segments_combined,
                 config,
                 args.output_plot,
                 places_coords_df) # Pass loaded places data

    print(f" Analyse abgeschlossen für: {base_filename}")


# ------------------------------------------------------------
#  Command Line Interface (ERWEITERT für optionalen Orts-Input)
# ------------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Analyze GPX track data from CSV, find peaks, calculate stats, generate plots, and optionally annotate with places.")

    # Input/Output Arguments
    parser.add_argument("--input-csv", required=True, help="Path to the input track data CSV file (from step 2).")
    parser.add_argument("--output-plot", required=True, help="Path to save the output plot PNG file.")
    parser.add_argument("--output-peak-csv", required=True, help="Path to save the peak and segment data CSV file.")
    parser.add_argument("--output-stats-csv", required=True, help="Path to save the overall statistics CSV file.")
    # NEU: Optionaler Input für Orte mit Koordinaten
    parser.add_argument("--places-coords-csv", help="Optional path to the CSV file with places and coordinates (output of step 8b).")


    # Configuration Arguments
    parser.add_argument("--smooth-window", type=int, default=11)
    parser.add_argument("--smooth-poly", type=int, default=2)
    parser.add_argument("--gain-threshold", type=float, default=30.0)
    parser.add_argument("--eps-height", type=float, default=0.3)
    parser.add_argument("--prominence", type=float, default=40.0)
    parser.add_argument("--peak-edge-km", type=float, default=0.25)
    parser.add_argument("--plot-dpi", type=int, default=150)
    parser.add_argument("--plot-x-tick-major", type=float, default=5.0)
    parser.add_argument("--plot-x-tick-minor", type=float, default=1.0)
    parser.add_argument("--pause-min-duration", type=float, default=120.0)
    parser.add_argument("--pause-max-distance", type=float, default=5.0)

    args = parser.parse_args()
    main(args)